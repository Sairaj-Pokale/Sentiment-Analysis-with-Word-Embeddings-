{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from sklearn.linear_model import Perceptron \n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('dataset//amazon_reviews_us_Office_Products_v1_00.tsv', sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_list = []\n",
    "# target_df = data[[\"star_rating\", \"review_body\"]]\n",
    "# target_df = target_df.dropna(subset=['review_body'])\n",
    "# for i in target_df[\"star_rating\"]:\n",
    "#     if (i == 5 or i == 4):\n",
    "#         class_list.append(\"pos\")\n",
    "#     elif (i == 1 or i == 2 or i ==3):\n",
    "#         class_list.append(\"neg\")\n",
    "#     else:\n",
    "#         try:\n",
    "#             if (int(i) == 5 or int(i) == 4):\n",
    "#                 class_list.append(\"pos\")\n",
    "#             elif (int(i) == 1 or int(i) == 2 or int(i) ==3):\n",
    "#                 class_list.append(\"neg\")\n",
    "#         except:\n",
    "#             class_list.append(\"XO\")\n",
    "# target_df[\"Classes\"] = class_list\n",
    "# rmd = [\"XO\"]\n",
    "# target_df = target_df[~target_df['Classes'].isin(rmd)]\n",
    "# neg_df = target_df.query('(Classes != \"pos\")').sample(n=50000)\n",
    "# pos_df = target_df.query('(Classes != \"neg\")').sample(n=50000)\n",
    "# frames = [neg_df, pos_df]\n",
    "# result = pd.concat(frames)\n",
    "# final_df = result.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df['label'] = final_df['Classes'].map({'pos':1, 'neg':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.astype({'star_rating': 'int'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.head()\n",
    "# final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.to_csv('dataset/reduced_100k_dataset.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/reduced_100k_dataset.txt', sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.43164062e-01, -7.71484375e-02, -1.03027344e-01, -1.07421875e-01,\n",
       "        1.18164062e-01, -1.07421875e-01, -1.14257812e-01,  2.56347656e-02,\n",
       "        1.11816406e-01,  4.85839844e-02, -9.71679688e-02, -3.43750000e-01,\n",
       "       -6.29882812e-02, -1.25000000e-01, -2.70996094e-02,  9.42382812e-02,\n",
       "       -1.87500000e-01, -5.34667969e-02,  6.25000000e-02, -3.05175781e-02,\n",
       "       -2.90527344e-02, -4.80957031e-02, -5.51757812e-02, -4.08203125e-01,\n",
       "        1.01318359e-02, -2.32421875e-01, -1.70898438e-01,  2.63671875e-01,\n",
       "        3.49609375e-01, -2.11914062e-01,  1.43554688e-01, -6.22558594e-03,\n",
       "       -2.25585938e-01, -1.05468750e-01, -1.16210938e-01,  1.23046875e-01,\n",
       "        3.06640625e-01, -4.88281250e-02, -9.57031250e-02,  1.99218750e-01,\n",
       "       -1.57226562e-01, -2.80761719e-02,  1.58203125e-01, -2.42919922e-02,\n",
       "        1.29882812e-01, -8.98437500e-02, -7.61718750e-02,  3.54003906e-02,\n",
       "       -3.06396484e-02,  1.52343750e-01,  5.24902344e-02,  1.60980225e-03,\n",
       "        5.56640625e-02,  3.95507812e-02, -7.71484375e-02, -7.12890625e-02,\n",
       "       -9.22851562e-02, -7.03125000e-02,  2.03125000e-01,  1.53198242e-02,\n",
       "        2.98828125e-01,  1.75781250e-01, -4.54101562e-02,  9.52148438e-02,\n",
       "        4.12597656e-02,  7.76367188e-02,  9.47265625e-02,  1.67968750e-01,\n",
       "        2.01171875e-01, -7.22656250e-02,  1.83593750e-01,  2.15820312e-01,\n",
       "       -2.38281250e-01,  1.04980469e-01, -1.66015625e-02,  2.40234375e-01,\n",
       "        1.67236328e-02, -4.56542969e-02,  1.68945312e-01,  1.85546875e-01,\n",
       "        2.47070312e-01, -1.02050781e-01,  8.49609375e-02, -1.04003906e-01,\n",
       "       -4.74609375e-01,  2.63671875e-01, -1.57226562e-01,  8.25195312e-02,\n",
       "        2.19726562e-01, -3.03955078e-02, -2.55859375e-01, -1.97265625e-01,\n",
       "       -9.27734375e-02, -1.28173828e-02,  1.26953125e-01,  3.54003906e-02,\n",
       "        5.88378906e-02,  3.27148438e-02,  1.72851562e-01, -2.08740234e-02,\n",
       "       -1.65039062e-01, -2.81250000e-01,  8.49609375e-02, -1.69921875e-01,\n",
       "        2.31445312e-01, -1.41601562e-01,  7.91015625e-02, -1.92382812e-01,\n",
       "        7.61718750e-02, -2.23632812e-01, -1.06811523e-02,  5.66406250e-02,\n",
       "        1.56250000e-01,  7.17773438e-02, -1.56250000e-01, -1.44531250e-01,\n",
       "       -8.30078125e-02, -1.21093750e-01,  5.63964844e-02,  2.61718750e-01,\n",
       "        7.95898438e-02, -1.28784180e-02, -2.00195312e-01, -4.39453125e-02,\n",
       "       -1.01562500e-01,  1.29882812e-01,  9.42382812e-02,  1.90429688e-02,\n",
       "        1.42578125e-01,  1.59179688e-01, -7.47070312e-02, -3.24218750e-01,\n",
       "       -2.07031250e-01,  4.80957031e-02, -4.19921875e-02,  9.22851562e-02,\n",
       "       -4.39453125e-02, -2.20703125e-01, -6.25000000e-02,  8.39843750e-02,\n",
       "        2.29492188e-01, -1.11816406e-01,  9.03320312e-02,  2.08984375e-01,\n",
       "       -2.28515625e-01,  2.87109375e-01, -1.38671875e-01,  1.85546875e-01,\n",
       "       -2.10937500e-01, -2.05078125e-01,  3.00781250e-01, -1.82617188e-01,\n",
       "        1.23046875e-01, -1.61132812e-01,  9.37500000e-02,  1.25976562e-01,\n",
       "       -8.10546875e-02,  9.15527344e-05,  1.53320312e-01, -8.10546875e-02,\n",
       "       -1.93359375e-01,  7.08007812e-03,  3.84765625e-01,  1.05957031e-01,\n",
       "       -1.09375000e-01, -1.13769531e-01,  9.13085938e-02, -1.92382812e-01,\n",
       "        6.12792969e-02,  6.07299805e-03, -4.54101562e-02,  6.25000000e-02,\n",
       "       -1.30859375e-01, -1.83593750e-01, -1.76757812e-01, -1.87500000e-01,\n",
       "        2.44140625e-01,  1.89453125e-01, -1.93359375e-01, -2.29492188e-02,\n",
       "        2.53906250e-02,  3.93676758e-03, -1.38671875e-01, -2.81250000e-01,\n",
       "       -1.80664062e-01,  8.69140625e-02,  3.17382812e-02,  2.55859375e-01,\n",
       "       -2.30468750e-01, -5.24902344e-02, -2.18200684e-03,  1.60156250e-01,\n",
       "        1.57226562e-01,  2.79296875e-01,  1.37695312e-01,  1.04492188e-01,\n",
       "       -1.18652344e-01, -5.81054688e-02, -7.32421875e-02,  1.04980469e-02,\n",
       "       -1.77734375e-01, -1.07421875e-01, -1.76757812e-01, -1.23046875e-01,\n",
       "       -1.69921875e-01, -1.34765625e-01,  6.39648438e-02,  1.22558594e-01,\n",
       "        1.95312500e-01, -4.94140625e-01, -3.90625000e-02, -3.19824219e-02,\n",
       "       -1.58691406e-02, -4.10156250e-02, -1.43554688e-01, -8.59375000e-02,\n",
       "       -7.95898438e-02,  2.46093750e-01, -1.77734375e-01,  2.05078125e-01,\n",
       "        5.32226562e-02, -2.51464844e-02,  2.14843750e-01,  2.12402344e-02,\n",
       "        9.76562500e-02, -2.16796875e-01,  2.85156250e-01, -1.19140625e-01,\n",
       "       -1.66992188e-01, -3.60107422e-03,  4.61425781e-02, -1.63085938e-01,\n",
       "       -2.53906250e-01,  1.89453125e-01, -7.51953125e-02, -5.39550781e-02,\n",
       "       -1.77734375e-01, -4.32128906e-02, -7.38525391e-03,  1.57226562e-01,\n",
       "        2.53906250e-01, -1.52343750e-01, -5.27343750e-02, -1.25000000e-01,\n",
       "        1.54296875e-01,  1.11816406e-01, -1.54418945e-02,  8.97216797e-03,\n",
       "       -5.63964844e-02, -2.58789062e-02,  1.93359375e-01,  5.22460938e-02,\n",
       "       -1.56250000e-02, -5.68847656e-02,  1.17187500e-01,  6.00585938e-02,\n",
       "       -2.64892578e-02, -1.39648438e-01, -7.27539062e-02, -5.00488281e-02,\n",
       "        2.97851562e-02, -9.61914062e-02, -1.60156250e-01, -1.41601562e-01,\n",
       "        2.17773438e-01,  2.55859375e-01, -4.58984375e-02,  1.17187500e-01,\n",
       "       -2.46093750e-01, -7.27539062e-02, -8.69140625e-02,  1.57226562e-01,\n",
       "       -1.88476562e-01,  4.39453125e-02, -5.55419922e-03,  6.93359375e-02,\n",
       "        1.42578125e-01, -1.20605469e-01, -1.04003906e-01, -3.41796875e-02,\n",
       "        1.82617188e-01, -1.29882812e-01,  1.63574219e-02,  3.20312500e-01,\n",
       "       -1.12304688e-01, -1.12915039e-02, -1.38671875e-01, -2.20703125e-01,\n",
       "        7.59124756e-04,  3.94531250e-01,  1.03515625e-01, -6.64062500e-02,\n",
       "       -2.67578125e-01, -2.47070312e-01, -7.27539062e-02,  1.07910156e-01,\n",
       "        1.18652344e-01, -8.30078125e-02,  6.54296875e-02, -2.94189453e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king = wv['king']\n",
    "vec_man = wv['man']\n",
    "vec_woman = wv['woman']\n",
    "vec_king\n",
    "vec_man\n",
    "vec_woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118191123008728)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['woman','king'], negative=['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baseball', 0.5879416465759277),\n",
       " ('cricket', 0.5189065933227539),\n",
       " ('bats', 0.5051103830337524),\n",
       " ('Football', 0.4850393831729889),\n",
       " ('soccer', 0.4830698072910309)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['football','bat'], negative=['foot'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7697915434837341),\n",
       " ('Berlin', 0.716059148311615),\n",
       " ('Frankfurt', 0.6547871232032776),\n",
       " ('Belgium', 0.6479434967041016),\n",
       " ('Cologne', 0.6293436884880066)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=[\"Paris\", \"Germany\"], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list=[]\n",
    "for i in df['review_body']:\n",
    "    testl = i.split(\" \")\n",
    "    training_list.append(testl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec(training_list, vector_size=300, min_count=9, window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = gensim.models.Word2Vec.load('dataset/training.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_model.wv['Excellent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad,', 0.6201503872871399),\n",
       " ('horrible', 0.5988957285881042),\n",
       " ('bad.', 0.5959342122077942)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.wv.most_similar(positive=['bad'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans: Pretrained model encodes similarities better simply because it covers a larger and clean vocab and since we our training the custom model on a limited dataset we are capping the number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>Classes</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Excellent product</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought the same calendar last year and was s...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I just finished scanning 1000's of documents a...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I have had it for several months now but hardl...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I love this pen!  Writes nice and looks beauti...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body Classes  \\\n",
       "0          4.0                                  Excellent product     pos   \n",
       "1          5.0  I bought the same calendar last year and was s...     pos   \n",
       "2          5.0  I just finished scanning 1000's of documents a...     pos   \n",
       "3          4.0  I have had it for several months now but hardl...     pos   \n",
       "4          5.0  I love this pen!  Writes nice and looks beauti...     pos   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating    100000\n",
       "review_body    100000\n",
       "Classes        100000\n",
       "label          100000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtf = df.review_body\n",
    "Ytf = df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_data = vectorizer.fit_transform(Xtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(tfidf_data, Ytf, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the tfidf classifier is 0.8193\n",
      "\n",
      " Confusion matrix\n",
      "[[8240 1817]\n",
      " [1797 8146]]\n",
      "\n",
      " The value of Precision 0.8176252132891699\n",
      "\n",
      " The value of Recall 0.8192698380770391\n",
      "\n",
      "F1 Score:  0.8184466994875916\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82     10057\n",
      "           1       0.82      0.82      0.82      9943\n",
      "\n",
      "    accuracy                           0.82     20000\n",
      "   macro avg       0.82      0.82      0.82     20000\n",
      "weighted avg       0.82      0.82      0.82     20000\n",
      "\n",
      "\n",
      "*****************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron \n",
    "from sklearn import metrics\n",
    "#tfidf Perceptron\n",
    "tf_pc = Perceptron(random_state=42, max_iter=2500).fit(xtrain, ytrain)\n",
    "tf_pc_predict = tf_pc.predict(xtest)\n",
    "print('\\n Accuracy of the tfidf classifier is',metrics.accuracy_score(ytest,tf_pc_predict))  #tfidf Perceptron\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,tf_pc_predict))\n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,tf_pc_predict))\n",
    "print('\\n The value of Recall', metrics.recall_score(ytest,tf_pc_predict))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(ytest, tf_pc_predict))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(ytest, tf_pc_predict))\n",
    "print(\"\\n*****************************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy of the tfidf classifier is 0.86825\n",
      "\n",
      " Confusion matrix\n",
      "[[8829 1228]\n",
      " [1407 8536]]\n",
      "\n",
      " The value of Precision 0.8742318721835314\n",
      "\n",
      " The value of Recall 0.8584934124509705\n",
      "\n",
      "F1 Score:  0.8662911655756836\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87     10057\n",
      "           1       0.87      0.86      0.87      9943\n",
      "\n",
      "    accuracy                           0.87     20000\n",
      "   macro avg       0.87      0.87      0.87     20000\n",
      "weighted avg       0.87      0.87      0.87     20000\n",
      "\n",
      "\n",
      "*****************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "#tfidf svm\n",
    "tf_svm = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto').fit(xtrain, ytrain)\n",
    "tf_svm_predict = tf_svm.predict(xtest)\n",
    "print('\\n Accuracy of the tfidf classifier is',metrics.accuracy_score(ytest,tf_svm_predict))  #tfidf svm\n",
    "print('\\n Confusion matrix')\n",
    "print(metrics.confusion_matrix(ytest,tf_svm_predict))\n",
    "print('\\n The value of Precision', metrics.precision_score(ytest,tf_svm_predict))\n",
    "print('\\n The value of Recall', metrics.recall_score(ytest,tf_svm_predict))\n",
    "print(\"\\nF1 Score: \",metrics.f1_score(ytest, tf_svm_predict))\n",
    "print(\"\\nClassification Report: \\n\",metrics.classification_report(ytest, tf_svm_predict))\n",
    "print(\"\\n*****************************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencevector(sentence): #averages the w2v vectors over the tokens\n",
    "    vectorsize = 300\n",
    "    wv_res = np.zeros(vectorsize)\n",
    "    count = 1\n",
    "    for w in sentence:\n",
    "        if w in wv:\n",
    "            count+=1\n",
    "            wv_res += wv[w]\n",
    "    wv_res = wv_res/count\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = df['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist = []\n",
    "for i in testdf:\n",
    "    newlist.append(i.split(\" \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token'] = newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vec'] = df['token'].apply(sentencevector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>Classes</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Excellent product</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[Excellent, product]</td>\n",
       "      <td>[-0.059407552083333336, 0.005045572916666667, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought the same calendar last year and was s...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, bought, the, same, calendar, last, year, a...</td>\n",
       "      <td>[0.045821295844184026, 0.027186923556857638, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I just finished scanning 1000's of documents a...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, just, finished, scanning, 1000's, of, docu...</td>\n",
       "      <td>[0.04669007467567374, 0.0354631406451584, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I have had it for several months now but hardl...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, have, had, it, for, several, months, now, ...</td>\n",
       "      <td>[0.041322905441810345, -0.013488440678037446, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I love this pen!  Writes nice and looks beauti...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, love, this, pen!, , Writes, nice, and, loo...</td>\n",
       "      <td>[0.05679886429398148, 0.030525843302408855, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Does not work with MFC-J875DW..... Cartridge w...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>[Does, not, work, with, MFC-J875DW....., Cartr...</td>\n",
       "      <td>[0.002399382044057377, 0.008637803499815895, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>too much detail for my hubby to do! ;(</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>[too, much, detail, for, my, hubby, to, do!, ;(]</td>\n",
       "      <td>[0.0379638671875, 0.012503487723214286, -0.042...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>this is hands-down the worst piece of *bleep* ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, hands-down, the, worst, piece, of, ...</td>\n",
       "      <td>[0.010974017894210446, 0.012256686814165346, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent!</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[Excellent!]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This product was great . I would recommend to ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[This, product, was, great, ., I, would, recom...</td>\n",
       "      <td>[0.01828182444852941, 0.016332289751838234, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body Classes  \\\n",
       "0          4.0                                  Excellent product     pos   \n",
       "1          5.0  I bought the same calendar last year and was s...     pos   \n",
       "2          5.0  I just finished scanning 1000's of documents a...     pos   \n",
       "3          4.0  I have had it for several months now but hardl...     pos   \n",
       "4          5.0  I love this pen!  Writes nice and looks beauti...     pos   \n",
       "5          1.0  Does not work with MFC-J875DW..... Cartridge w...     neg   \n",
       "6          3.0             too much detail for my hubby to do! ;(     neg   \n",
       "7          1.0  this is hands-down the worst piece of *bleep* ...     neg   \n",
       "8          5.0                                         Excellent!     pos   \n",
       "9          5.0  This product was great . I would recommend to ...     pos   \n",
       "\n",
       "   label                                              token  \\\n",
       "0      1                               [Excellent, product]   \n",
       "1      1  [I, bought, the, same, calendar, last, year, a...   \n",
       "2      1  [I, just, finished, scanning, 1000's, of, docu...   \n",
       "3      1  [I, have, had, it, for, several, months, now, ...   \n",
       "4      1  [I, love, this, pen!, , Writes, nice, and, loo...   \n",
       "5      0  [Does, not, work, with, MFC-J875DW....., Cartr...   \n",
       "6      0   [too, much, detail, for, my, hubby, to, do!, ;(]   \n",
       "7      0  [this, is, hands-down, the, worst, piece, of, ...   \n",
       "8      1                                       [Excellent!]   \n",
       "9      1  [This, product, was, great, ., I, would, recom...   \n",
       "\n",
       "                                                 vec  \n",
       "0  [-0.059407552083333336, 0.005045572916666667, ...  \n",
       "1  [0.045821295844184026, 0.027186923556857638, 0...  \n",
       "2  [0.04669007467567374, 0.0354631406451584, 0.01...  \n",
       "3  [0.041322905441810345, -0.013488440678037446, ...  \n",
       "4  [0.05679886429398148, 0.030525843302408855, 0....  \n",
       "5  [0.002399382044057377, 0.008637803499815895, 0...  \n",
       "6  [0.0379638671875, 0.012503487723214286, -0.042...  \n",
       "7  [0.010974017894210446, 0.012256686814165346, 0...  \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9  [0.01828182444852941, 0.016332289751838234, 0....  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['vec'].to_list()\n",
    "Y = df['label'].to_list()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_pc = Perceptron() #w2v perceptron\n",
    "wordvec_pc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_pred = wordvec_pc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77     10000\n",
      "           1       0.76      0.85      0.80     10000\n",
      "\n",
      "    accuracy                           0.79     20000\n",
      "   macro avg       0.79      0.79      0.79     20000\n",
      "weighted avg       0.79      0.79      0.79     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y_test,pcp_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(gamma=&#x27;auto&#x27;, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(gamma=&#x27;auto&#x27;, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(gamma='auto', kernel='linear')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm #w2v svm\n",
    "\n",
    "wordvec_svm = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "wordvec_svm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pred = wordvec_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.82     10000\n",
      "           1       0.84      0.76      0.80     10000\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.81      0.81      0.81     20000\n",
      "weighted avg       0.81      0.81      0.81     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(Y_test,svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained 2Word2Vec features)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: As showcased in the accuracies, TFIDF feature trained simple models outperforms word2vec feature trained simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['vec'].to_list()\n",
    "Y = df['label'].to_list()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Define MLP model\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 4A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saira\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "input_size =  300\n",
    "hidden1_size = 50\n",
    "hidden2_size = 5\n",
    "output_size =  2\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "model = TextClassifier(input_size, hidden1_size, hidden2_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saira\\AppData\\Local\\Temp\\ipykernel_15812\\1957741604.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  x_train = torch.Tensor(X_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 0.4580857426941395\n",
      "Epoch [2/10] Loss: 0.42277758510708807\n",
      "Epoch [3/10] Loss: 0.4122877170741558\n",
      "Epoch [4/10] Loss: 0.4055768816888332\n",
      "Epoch [5/10] Loss: 0.39900410665273667\n",
      "Epoch [6/10] Loss: 0.39428163250088694\n",
      "Epoch [7/10] Loss: 0.3900630583643913\n",
      "Epoch [8/10] Loss: 0.3851691832661629\n",
      "Epoch [9/10] Loss: 0.3806895156562328\n",
      "Epoch [10/10] Loss: 0.37670534210801127\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.Tensor(X_train)\n",
    "y_train = torch.LongTensor(Y_train)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "model_checkpoint_path = '/task4bmodel.pth' \n",
    "torch.save(model.state_dict(), model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkaccuracy(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum().item()\n",
    "    total = y_true.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 82.33%\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(input_size, hidden1_size, hidden2_size, output_size)\n",
    "model.load_state_dict(torch.load('/task4bmodel.pth'))  \n",
    "model.eval()\n",
    "\n",
    "x_test = torch.Tensor(X_test)\n",
    "y_test = torch.LongTensor(Y_test)\n",
    "\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(x_test)\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "\n",
    "accuracy = checkaccuracy(y_test, predicted_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concated vectors\n",
    "nlist = []\n",
    "for tokens in df['token']: #Concatenates the first 10 valid w2v vectors\n",
    "    word_vectors=[]\n",
    "    for w in tokens:\n",
    "        if w in wv:\n",
    "            word_vectors.append(wv[w])\n",
    "        else:\n",
    "            continue\n",
    "    while len(word_vectors) < 10:\n",
    "        word_vectors.append(np.zeros(300))\n",
    "    concatenated_vectors = np.concatenate(word_vectors[:10])\n",
    "    nlist.append(concatenated_vectors)\n",
    "\n",
    "    #break\n",
    "    #nlist.append(concatenated_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cncvec'] = nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>Classes</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "      <th>vec</th>\n",
       "      <th>cncvec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Excellent product</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[Excellent, product]</td>\n",
       "      <td>[-0.059407552083333336, 0.005045572916666667, ...</td>\n",
       "      <td>[-0.11669921875, -0.080078125, -0.1923828125, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought the same calendar last year and was s...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, bought, the, same, calendar, last, year, a...</td>\n",
       "      <td>[0.045821295844184026, 0.027186923556857638, 0...</td>\n",
       "      <td>[0.07910156, -0.0050354004, 0.111816406, 0.212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I just finished scanning 1000's of documents a...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, just, finished, scanning, 1000's, of, docu...</td>\n",
       "      <td>[0.04669007467567374, 0.0354631406451584, 0.01...</td>\n",
       "      <td>[0.07910156, -0.0050354004, 0.111816406, 0.212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I have had it for several months now but hardl...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, have, had, it, for, several, months, now, ...</td>\n",
       "      <td>[0.041322905441810345, -0.013488440678037446, ...</td>\n",
       "      <td>[0.07910156, -0.0050354004, 0.111816406, 0.212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I love this pen!  Writes nice and looks beauti...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, love, this, pen!, , Writes, nice, and, loo...</td>\n",
       "      <td>[0.05679886429398148, 0.030525843302408855, 0....</td>\n",
       "      <td>[0.07910156, -0.0050354004, 0.111816406, 0.212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Does not work with MFC-J875DW..... Cartridge w...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>[Does, not, work, with, MFC-J875DW....., Cartr...</td>\n",
       "      <td>[0.002399382044057377, 0.008637803499815895, 0...</td>\n",
       "      <td>[0.24121094, 0.09863281, 0.076171875, 0.308593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>too much detail for my hubby to do! ;(</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>[too, much, detail, for, my, hubby, to, do!, ;(]</td>\n",
       "      <td>[0.0379638671875, 0.012503487723214286, -0.042...</td>\n",
       "      <td>[0.1298828125, 0.1318359375, -0.032958984375, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>this is hands-down the worst piece of *bleep* ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, hands-down, the, worst, piece, of, ...</td>\n",
       "      <td>[0.010974017894210446, 0.012256686814165346, 0...</td>\n",
       "      <td>[0.109375, 0.140625, -0.03173828, 0.16601562, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent!</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[Excellent!]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This product was great . I would recommend to ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[This, product, was, great, ., I, would, recom...</td>\n",
       "      <td>[0.01828182444852941, 0.016332289751838234, 0....</td>\n",
       "      <td>[-0.2890625, 0.19921875, 0.16015625, 0.0252685...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body Classes  \\\n",
       "0          4.0                                  Excellent product     pos   \n",
       "1          5.0  I bought the same calendar last year and was s...     pos   \n",
       "2          5.0  I just finished scanning 1000's of documents a...     pos   \n",
       "3          4.0  I have had it for several months now but hardl...     pos   \n",
       "4          5.0  I love this pen!  Writes nice and looks beauti...     pos   \n",
       "5          1.0  Does not work with MFC-J875DW..... Cartridge w...     neg   \n",
       "6          3.0             too much detail for my hubby to do! ;(     neg   \n",
       "7          1.0  this is hands-down the worst piece of *bleep* ...     neg   \n",
       "8          5.0                                         Excellent!     pos   \n",
       "9          5.0  This product was great . I would recommend to ...     pos   \n",
       "\n",
       "   label                                              token  \\\n",
       "0      1                               [Excellent, product]   \n",
       "1      1  [I, bought, the, same, calendar, last, year, a...   \n",
       "2      1  [I, just, finished, scanning, 1000's, of, docu...   \n",
       "3      1  [I, have, had, it, for, several, months, now, ...   \n",
       "4      1  [I, love, this, pen!, , Writes, nice, and, loo...   \n",
       "5      0  [Does, not, work, with, MFC-J875DW....., Cartr...   \n",
       "6      0   [too, much, detail, for, my, hubby, to, do!, ;(]   \n",
       "7      0  [this, is, hands-down, the, worst, piece, of, ...   \n",
       "8      1                                       [Excellent!]   \n",
       "9      1  [This, product, was, great, ., I, would, recom...   \n",
       "\n",
       "                                                 vec  \\\n",
       "0  [-0.059407552083333336, 0.005045572916666667, ...   \n",
       "1  [0.045821295844184026, 0.027186923556857638, 0...   \n",
       "2  [0.04669007467567374, 0.0354631406451584, 0.01...   \n",
       "3  [0.041322905441810345, -0.013488440678037446, ...   \n",
       "4  [0.05679886429398148, 0.030525843302408855, 0....   \n",
       "5  [0.002399382044057377, 0.008637803499815895, 0...   \n",
       "6  [0.0379638671875, 0.012503487723214286, -0.042...   \n",
       "7  [0.010974017894210446, 0.012256686814165346, 0...   \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  [0.01828182444852941, 0.016332289751838234, 0....   \n",
       "\n",
       "                                              cncvec  \n",
       "0  [-0.11669921875, -0.080078125, -0.1923828125, ...  \n",
       "1  [0.07910156, -0.0050354004, 0.111816406, 0.212...  \n",
       "2  [0.07910156, -0.0050354004, 0.111816406, 0.212...  \n",
       "3  [0.07910156, -0.0050354004, 0.111816406, 0.212...  \n",
       "4  [0.07910156, -0.0050354004, 0.111816406, 0.212...  \n",
       "5  [0.24121094, 0.09863281, 0.076171875, 0.308593...  \n",
       "6  [0.1298828125, 0.1318359375, -0.032958984375, ...  \n",
       "7  [0.109375, 0.140625, -0.03173828, 0.16601562, ...  \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "9  [-0.2890625, 0.19921875, 0.16015625, 0.0252685...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df['cncvec'].to_list()\n",
    "Y1 = df['label'].to_list()\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.2, stratify=Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size =  3000\n",
    "hidden1_size = 50\n",
    "hidden2_size = 5\n",
    "output_size =  2\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "model = TextClassifier(input_size, hidden1_size, hidden2_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 0.5292904435515404\n",
      "Epoch [2/10] Loss: 0.47959192517995836\n",
      "Epoch [3/10] Loss: 0.4408720701515675\n",
      "Epoch [4/10] Loss: 0.39792831239700316\n",
      "Epoch [5/10] Loss: 0.35201616400927305\n",
      "Epoch [6/10] Loss: 0.3053746315151453\n",
      "Epoch [7/10] Loss: 0.2640626740306616\n",
      "Epoch [8/10] Loss: 0.22757717830687763\n",
      "Epoch [9/10] Loss: 0.20016998102217912\n",
      "Epoch [10/10] Loss: 0.17641350098326802\n"
     ]
    }
   ],
   "source": [
    "x1_train = torch.Tensor(X1_train)\n",
    "y1_train = torch.LongTensor(Y1_train)\n",
    "\n",
    "train_dataset = TensorDataset(x1_train, y1_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "model_checkpoint_path = '/task4bbmodel.pth'  \n",
    "torch.save(model.state_dict(), model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 72.69%\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(input_size, hidden1_size, hidden2_size, output_size)\n",
    "model.load_state_dict(torch.load('/task4bbmodel.pth'))  \n",
    "model.eval()\n",
    "\n",
    "x1_test = torch.Tensor(X1_test)\n",
    "y1_test = torch.LongTensor(Y1_test)\n",
    "\n",
    "test_dataset = TensorDataset(x1_test, y1_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(x1_test)\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "\n",
    "accuracy = checkaccuracy(y1_test, predicted_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: SVM with TfIdf retains the highest accuracy and MLP with concatenated w2v features retains the lowest accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnlist = [] \n",
    "for tokens in df['token']: #limits maximum review length at 10 and pads and truncates as per requirement\n",
    "    word_vectors=[]\n",
    "    for w in tokens:\n",
    "        if w in wv:\n",
    "            word_vectors.append(wv[w])\n",
    "        else:\n",
    "            continue\n",
    "    while len(word_vectors) < 10:\n",
    "        word_vectors.append(np.zeros(300))\n",
    "    rnnlist.append(word_vectors[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rnnvec'] = rnnlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>Classes</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "      <th>vec</th>\n",
       "      <th>cncvec</th>\n",
       "      <th>rnnvec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Excellent product</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[Excellent, product]</td>\n",
       "      <td>[-0.059407552083333336, 0.005045572916666667, ...</td>\n",
       "      <td>[-0.11669921875, -0.080078125, -0.1923828125, ...</td>\n",
       "      <td>[[-0.11669922, -0.080078125, -0.19238281, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought the same calendar last year and was s...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, bought, the, same, calendar, last, year, a...</td>\n",
       "      <td>[0.045821295844184026, 0.027186923556857638, 0...</td>\n",
       "      <td>[0.07910156, -0.0050354004, 0.111816406, 0.212...</td>\n",
       "      <td>[[0.07910156, -0.0050354004, 0.111816406, 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I just finished scanning 1000's of documents a...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, just, finished, scanning, 1000's, of, docu...</td>\n",
       "      <td>[0.04669007467567374, 0.0354631406451584, 0.01...</td>\n",
       "      <td>[0.07910156, -0.0050354004, 0.111816406, 0.212...</td>\n",
       "      <td>[[0.07910156, -0.0050354004, 0.111816406, 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I have had it for several months now but hardl...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, have, had, it, for, several, months, now, ...</td>\n",
       "      <td>[0.041322905441810345, -0.013488440678037446, ...</td>\n",
       "      <td>[0.07910156, -0.0050354004, 0.111816406, 0.212...</td>\n",
       "      <td>[[0.07910156, -0.0050354004, 0.111816406, 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I love this pen!  Writes nice and looks beauti...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, love, this, pen!, , Writes, nice, and, loo...</td>\n",
       "      <td>[0.05679886429398148, 0.030525843302408855, 0....</td>\n",
       "      <td>[0.07910156, -0.0050354004, 0.111816406, 0.212...</td>\n",
       "      <td>[[0.07910156, -0.0050354004, 0.111816406, 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Does not work with MFC-J875DW..... Cartridge w...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>[Does, not, work, with, MFC-J875DW....., Cartr...</td>\n",
       "      <td>[0.002399382044057377, 0.008637803499815895, 0...</td>\n",
       "      <td>[0.24121094, 0.09863281, 0.076171875, 0.308593...</td>\n",
       "      <td>[[0.24121094, 0.09863281, 0.076171875, 0.30859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>too much detail for my hubby to do! ;(</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>[too, much, detail, for, my, hubby, to, do!, ;(]</td>\n",
       "      <td>[0.0379638671875, 0.012503487723214286, -0.042...</td>\n",
       "      <td>[0.1298828125, 0.1318359375, -0.032958984375, ...</td>\n",
       "      <td>[[0.12988281, 0.13183594, -0.032958984, 0.1484...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>this is hands-down the worst piece of *bleep* ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, hands-down, the, worst, piece, of, ...</td>\n",
       "      <td>[0.010974017894210446, 0.012256686814165346, 0...</td>\n",
       "      <td>[0.109375, 0.140625, -0.03173828, 0.16601562, ...</td>\n",
       "      <td>[[0.109375, 0.140625, -0.03173828, 0.16601562,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent!</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[Excellent!]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This product was great . I would recommend to ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "      <td>[This, product, was, great, ., I, would, recom...</td>\n",
       "      <td>[0.01828182444852941, 0.016332289751838234, 0....</td>\n",
       "      <td>[-0.2890625, 0.19921875, 0.16015625, 0.0252685...</td>\n",
       "      <td>[[-0.2890625, 0.19921875, 0.16015625, 0.025268...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                        review_body Classes  \\\n",
       "0          4.0                                  Excellent product     pos   \n",
       "1          5.0  I bought the same calendar last year and was s...     pos   \n",
       "2          5.0  I just finished scanning 1000's of documents a...     pos   \n",
       "3          4.0  I have had it for several months now but hardl...     pos   \n",
       "4          5.0  I love this pen!  Writes nice and looks beauti...     pos   \n",
       "5          1.0  Does not work with MFC-J875DW..... Cartridge w...     neg   \n",
       "6          3.0             too much detail for my hubby to do! ;(     neg   \n",
       "7          1.0  this is hands-down the worst piece of *bleep* ...     neg   \n",
       "8          5.0                                         Excellent!     pos   \n",
       "9          5.0  This product was great . I would recommend to ...     pos   \n",
       "\n",
       "   label                                              token  \\\n",
       "0      1                               [Excellent, product]   \n",
       "1      1  [I, bought, the, same, calendar, last, year, a...   \n",
       "2      1  [I, just, finished, scanning, 1000's, of, docu...   \n",
       "3      1  [I, have, had, it, for, several, months, now, ...   \n",
       "4      1  [I, love, this, pen!, , Writes, nice, and, loo...   \n",
       "5      0  [Does, not, work, with, MFC-J875DW....., Cartr...   \n",
       "6      0   [too, much, detail, for, my, hubby, to, do!, ;(]   \n",
       "7      0  [this, is, hands-down, the, worst, piece, of, ...   \n",
       "8      1                                       [Excellent!]   \n",
       "9      1  [This, product, was, great, ., I, would, recom...   \n",
       "\n",
       "                                                 vec  \\\n",
       "0  [-0.059407552083333336, 0.005045572916666667, ...   \n",
       "1  [0.045821295844184026, 0.027186923556857638, 0...   \n",
       "2  [0.04669007467567374, 0.0354631406451584, 0.01...   \n",
       "3  [0.041322905441810345, -0.013488440678037446, ...   \n",
       "4  [0.05679886429398148, 0.030525843302408855, 0....   \n",
       "5  [0.002399382044057377, 0.008637803499815895, 0...   \n",
       "6  [0.0379638671875, 0.012503487723214286, -0.042...   \n",
       "7  [0.010974017894210446, 0.012256686814165346, 0...   \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  [0.01828182444852941, 0.016332289751838234, 0....   \n",
       "\n",
       "                                              cncvec  \\\n",
       "0  [-0.11669921875, -0.080078125, -0.1923828125, ...   \n",
       "1  [0.07910156, -0.0050354004, 0.111816406, 0.212...   \n",
       "2  [0.07910156, -0.0050354004, 0.111816406, 0.212...   \n",
       "3  [0.07910156, -0.0050354004, 0.111816406, 0.212...   \n",
       "4  [0.07910156, -0.0050354004, 0.111816406, 0.212...   \n",
       "5  [0.24121094, 0.09863281, 0.076171875, 0.308593...   \n",
       "6  [0.1298828125, 0.1318359375, -0.032958984375, ...   \n",
       "7  [0.109375, 0.140625, -0.03173828, 0.16601562, ...   \n",
       "8  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "9  [-0.2890625, 0.19921875, 0.16015625, 0.0252685...   \n",
       "\n",
       "                                              rnnvec  \n",
       "0  [[-0.11669922, -0.080078125, -0.19238281, 0.02...  \n",
       "1  [[0.07910156, -0.0050354004, 0.111816406, 0.21...  \n",
       "2  [[0.07910156, -0.0050354004, 0.111816406, 0.21...  \n",
       "3  [[0.07910156, -0.0050354004, 0.111816406, 0.21...  \n",
       "4  [[0.07910156, -0.0050354004, 0.111816406, 0.21...  \n",
       "5  [[0.24121094, 0.09863281, 0.076171875, 0.30859...  \n",
       "6  [[0.12988281, 0.13183594, -0.032958984, 0.1484...  \n",
       "7  [[0.109375, 0.140625, -0.03173828, 0.16601562,...  \n",
       "8  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "9  [[-0.2890625, 0.19921875, 0.16015625, 0.025268...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim,  batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        output, hidden = self.rnn(text)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 300 \n",
    "hidden_dim = 10\n",
    "output_dim = 1  \n",
    "\n",
    "model = RNNClassifier(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df['rnnvec'].to_list()\n",
    "Y2 = df['label'].to_list()\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, test_size=0.2, stratify=Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Train Loss: 0.6746\n",
      "Epoch 02, Train Loss: 0.6063\n",
      "Epoch 03, Train Loss: 0.5637\n",
      "Epoch 04, Train Loss: 0.5476\n",
      "Epoch 05, Train Loss: 0.5382\n",
      "Epoch 06, Train Loss: 0.5309\n",
      "Epoch 07, Train Loss: 0.5260\n",
      "Epoch 08, Train Loss: 0.5217\n",
      "Epoch 09, Train Loss: 0.5186\n",
      "Epoch 10, Train Loss: 0.5156\n",
      "Epoch 11, Train Loss: 0.5133\n",
      "Epoch 12, Train Loss: 0.5110\n",
      "Epoch 13, Train Loss: 0.5090\n",
      "Epoch 14, Train Loss: 0.5071\n",
      "Epoch 15, Train Loss: 0.5050\n",
      "Epoch 16, Train Loss: 0.5036\n",
      "Epoch 17, Train Loss: 0.5022\n",
      "Epoch 18, Train Loss: 0.5006\n",
      "Epoch 19, Train Loss: 0.4991\n",
      "Epoch 20, Train Loss: 0.4978\n",
      "Epoch 21, Train Loss: 0.4966\n",
      "Epoch 22, Train Loss: 0.4952\n",
      "Epoch 23, Train Loss: 0.4945\n",
      "Epoch 24, Train Loss: 0.4929\n",
      "Epoch 25, Train Loss: 0.4923\n",
      "Epoch 26, Train Loss: 0.4912\n",
      "Epoch 27, Train Loss: 0.4895\n",
      "Epoch 28, Train Loss: 0.4889\n",
      "Epoch 29, Train Loss: 0.4882\n",
      "Epoch 30, Train Loss: 0.4872\n",
      "Epoch 31, Train Loss: 0.4862\n",
      "Epoch 32, Train Loss: 0.4849\n",
      "Epoch 33, Train Loss: 0.4838\n",
      "Epoch 34, Train Loss: 0.4833\n",
      "Epoch 35, Train Loss: 0.4828\n",
      "Epoch 36, Train Loss: 0.4811\n",
      "Epoch 37, Train Loss: 0.4807\n",
      "Epoch 38, Train Loss: 0.4801\n",
      "Epoch 39, Train Loss: 0.4790\n",
      "Epoch 40, Train Loss: 0.4780\n",
      "Epoch 41, Train Loss: 0.4771\n",
      "Epoch 42, Train Loss: 0.4762\n",
      "Epoch 43, Train Loss: 0.4758\n",
      "Epoch 44, Train Loss: 0.4745\n",
      "Epoch 45, Train Loss: 0.4736\n",
      "Epoch 46, Train Loss: 0.4731\n",
      "Epoch 47, Train Loss: 0.4725\n",
      "Epoch 48, Train Loss: 0.4717\n",
      "Epoch 49, Train Loss: 0.4703\n",
      "Epoch 50, Train Loss: 0.4702\n"
     ]
    }
   ],
   "source": [
    "x2_train = torch.Tensor(X2_train)\n",
    "y2_train = torch.FloatTensor(Y2_train)\n",
    "train_dataset = TensorDataset(x2_train, y2_train)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"BATCH0: \", batch[0].shape)\n",
    "        # print(\"BATCH1 \", batch[1].shape)\n",
    "        predictions = model(batch[0]).squeeze(0)\n",
    "        #print(\"Predictions shape: \", predictions.shape)\n",
    "        loss = criterion(predictions.squeeze(1), batch[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training the model\n",
    "N_EPOCHS = 50\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1:02}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "\n",
    "model_checkpoint_path = '/task5amodel.pth'\n",
    "torch.save(model.state_dict(), model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.87%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[0]).squeeze(0)\n",
    "            predicted_labels = (predictions.squeeze(1) > 0.5).float()\n",
    "            correct_preds += (predicted_labels == batch[1]).sum().item()\n",
    "            total_preds += batch[1].size(0)\n",
    "    \n",
    "    accuracy = correct_preds / total_preds\n",
    "    return accuracy\n",
    "\n",
    "x2_test = torch.Tensor(X2_test)\n",
    "y2_test = torch.FloatTensor(Y2_test)\n",
    "test_dataset = TensorDataset(x2_test, y2_test)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "model = RNNClassifier(input_dim=300, hidden_dim=10, output_dim=1)\n",
    "\n",
    "# Load the trained model's weights\n",
    "model.load_state_dict(torch.load('/task5amodel.pth'))\n",
    "\n",
    "\n",
    "test_accuracy = evaluate(model, test_loader)  \n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        output, hidden = self.gru(text)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 300  \n",
    "hidden_dim = 10\n",
    "output_dim = 1  \n",
    "\n",
    "model = GRUClassifier(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Train Loss: 0.6917\n",
      "Epoch 02, Train Loss: 0.6800\n",
      "Epoch 03, Train Loss: 0.6673\n",
      "Epoch 04, Train Loss: 0.6493\n",
      "Epoch 05, Train Loss: 0.6096\n",
      "Epoch 06, Train Loss: 0.5649\n",
      "Epoch 07, Train Loss: 0.5440\n",
      "Epoch 08, Train Loss: 0.5321\n",
      "Epoch 09, Train Loss: 0.5237\n",
      "Epoch 10, Train Loss: 0.5174\n",
      "Epoch 11, Train Loss: 0.5125\n",
      "Epoch 12, Train Loss: 0.5085\n",
      "Epoch 13, Train Loss: 0.5052\n",
      "Epoch 14, Train Loss: 0.5020\n",
      "Epoch 15, Train Loss: 0.4995\n",
      "Epoch 16, Train Loss: 0.4973\n",
      "Epoch 17, Train Loss: 0.4951\n",
      "Epoch 18, Train Loss: 0.4930\n",
      "Epoch 19, Train Loss: 0.4912\n",
      "Epoch 20, Train Loss: 0.4894\n",
      "Epoch 21, Train Loss: 0.4877\n",
      "Epoch 22, Train Loss: 0.4861\n",
      "Epoch 23, Train Loss: 0.4847\n",
      "Epoch 24, Train Loss: 0.4831\n",
      "Epoch 25, Train Loss: 0.4816\n",
      "Epoch 26, Train Loss: 0.4800\n",
      "Epoch 27, Train Loss: 0.4787\n",
      "Epoch 28, Train Loss: 0.4777\n",
      "Epoch 29, Train Loss: 0.4764\n",
      "Epoch 30, Train Loss: 0.4749\n",
      "Epoch 31, Train Loss: 0.4737\n",
      "Epoch 32, Train Loss: 0.4725\n",
      "Epoch 33, Train Loss: 0.4712\n",
      "Epoch 34, Train Loss: 0.4700\n",
      "Epoch 35, Train Loss: 0.4688\n",
      "Epoch 36, Train Loss: 0.4679\n",
      "Epoch 37, Train Loss: 0.4669\n",
      "Epoch 38, Train Loss: 0.4660\n",
      "Epoch 39, Train Loss: 0.4650\n",
      "Epoch 40, Train Loss: 0.4640\n",
      "Epoch 41, Train Loss: 0.4629\n",
      "Epoch 42, Train Loss: 0.4623\n",
      "Epoch 43, Train Loss: 0.4613\n",
      "Epoch 44, Train Loss: 0.4602\n",
      "Epoch 45, Train Loss: 0.4596\n",
      "Epoch 46, Train Loss: 0.4587\n",
      "Epoch 47, Train Loss: 0.4577\n",
      "Epoch 48, Train Loss: 0.4570\n",
      "Epoch 49, Train Loss: 0.4560\n",
      "Epoch 50, Train Loss: 0.4556\n"
     ]
    }
   ],
   "source": [
    "x2_train = torch.Tensor(X2_train)\n",
    "y2_train = torch.FloatTensor(Y2_train)\n",
    "train_dataset = TensorDataset(x2_train, y2_train)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"BATCH0: \", batch[0].shape)\n",
    "        # print(\"BATCH1 \", batch[1].shape)\n",
    "        predictions = model(batch[0]).squeeze(0)\n",
    "        #print(\"Predictions shape: \", predictions.shape)\n",
    "        loss = criterion(predictions.squeeze(1), batch[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "N_EPOCHS = 50\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1:02}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "\n",
    "model_checkpoint_path = '/task5bmodel.pth'\n",
    "torch.save(model.state_dict(), model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.18%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[0]).squeeze(0)\n",
    "            predicted_labels = (predictions.squeeze(1) > 0.5).float()\n",
    "            correct_preds += (predicted_labels == batch[1]).sum().item()\n",
    "            total_preds += batch[1].size(0)\n",
    "    \n",
    "    accuracy = correct_preds / total_preds\n",
    "    return accuracy\n",
    "\n",
    "x2_test = torch.Tensor(X2_test)\n",
    "y2_test = torch.FloatTensor(Y2_test)\n",
    "test_dataset = TensorDataset(x2_test, y2_test)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "model = GRUClassifier(input_dim=300, hidden_dim=10, output_dim=1)\n",
    "\n",
    "# Load the trained model's weights\n",
    "model.load_state_dict(torch.load('/task5bmodel.pth'))\n",
    "\n",
    "\n",
    "test_accuracy = evaluate(model, test_loader)  \n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        output, (hidden, cell) = self.lstm(text)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 300 \n",
    "hidden_dim = 10\n",
    "output_dim = 1  \n",
    "\n",
    "model = LSTMClassifier(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Train Loss: 0.6932\n",
      "Epoch 02, Train Loss: 0.6877\n",
      "Epoch 03, Train Loss: 0.6795\n",
      "Epoch 04, Train Loss: 0.6533\n",
      "Epoch 05, Train Loss: 0.5870\n",
      "Epoch 06, Train Loss: 0.5557\n",
      "Epoch 07, Train Loss: 0.5414\n",
      "Epoch 08, Train Loss: 0.5323\n",
      "Epoch 09, Train Loss: 0.5261\n",
      "Epoch 10, Train Loss: 0.5210\n",
      "Epoch 11, Train Loss: 0.5165\n",
      "Epoch 12, Train Loss: 0.5128\n",
      "Epoch 13, Train Loss: 0.5097\n",
      "Epoch 14, Train Loss: 0.5069\n",
      "Epoch 15, Train Loss: 0.5040\n",
      "Epoch 16, Train Loss: 0.5016\n",
      "Epoch 17, Train Loss: 0.4989\n",
      "Epoch 18, Train Loss: 0.4969\n",
      "Epoch 19, Train Loss: 0.4947\n",
      "Epoch 20, Train Loss: 0.4925\n",
      "Epoch 21, Train Loss: 0.4908\n",
      "Epoch 22, Train Loss: 0.4887\n",
      "Epoch 23, Train Loss: 0.4871\n",
      "Epoch 24, Train Loss: 0.4855\n",
      "Epoch 25, Train Loss: 0.4837\n",
      "Epoch 26, Train Loss: 0.4822\n",
      "Epoch 27, Train Loss: 0.4806\n",
      "Epoch 28, Train Loss: 0.4792\n",
      "Epoch 29, Train Loss: 0.4780\n",
      "Epoch 30, Train Loss: 0.4765\n",
      "Epoch 31, Train Loss: 0.4754\n",
      "Epoch 32, Train Loss: 0.4743\n",
      "Epoch 33, Train Loss: 0.4730\n",
      "Epoch 34, Train Loss: 0.4718\n",
      "Epoch 35, Train Loss: 0.4708\n",
      "Epoch 36, Train Loss: 0.4698\n",
      "Epoch 37, Train Loss: 0.4684\n",
      "Epoch 38, Train Loss: 0.4675\n",
      "Epoch 39, Train Loss: 0.4667\n",
      "Epoch 40, Train Loss: 0.4655\n",
      "Epoch 41, Train Loss: 0.4648\n",
      "Epoch 42, Train Loss: 0.4637\n",
      "Epoch 43, Train Loss: 0.4631\n",
      "Epoch 44, Train Loss: 0.4620\n",
      "Epoch 45, Train Loss: 0.4614\n",
      "Epoch 46, Train Loss: 0.4604\n",
      "Epoch 47, Train Loss: 0.4596\n",
      "Epoch 48, Train Loss: 0.4588\n",
      "Epoch 49, Train Loss: 0.4576\n",
      "Epoch 50, Train Loss: 0.4572\n"
     ]
    }
   ],
   "source": [
    "x2_train = torch.Tensor(X2_train)\n",
    "y2_train = torch.FloatTensor(Y2_train)\n",
    "train_dataset = TensorDataset(x2_train, y2_train)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"BATCH0: \", batch[0].shape)\n",
    "        # print(\"BATCH1 \", batch[1].shape)\n",
    "        predictions = model(batch[0]).squeeze(0)\n",
    "        #print(\"Predictions shape: \", predictions.shape)\n",
    "        loss = criterion(predictions.squeeze(1), batch[1])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "N_EPOCHS = 50\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1:02}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "model_checkpoint_path = '/task5cmodel.pth'\n",
    "torch.save(model.state_dict(), model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.08%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[0]).squeeze(0)\n",
    "            predicted_labels = (predictions.squeeze(1) > 0.5).float()\n",
    "            correct_preds += (predicted_labels == batch[1]).sum().item()\n",
    "            total_preds += batch[1].size(0)\n",
    "    \n",
    "    accuracy = correct_preds / total_preds\n",
    "    return accuracy\n",
    "\n",
    "x2_test = torch.Tensor(X2_test)\n",
    "y2_test = torch.FloatTensor(Y2_test)\n",
    "test_dataset = TensorDataset(x2_test, y2_test)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "model = LSTMClassifier(input_dim=300, hidden_dim=10, output_dim=1)\n",
    "\n",
    "# Load the trained model's weights\n",
    "model.load_state_dict(torch.load('/task5cmodel.pth'))\n",
    "\n",
    "\n",
    "test_accuracy = evaluate(model, test_loader)  \n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: Accuracies for the 3 models lie in the same range 74~76"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
